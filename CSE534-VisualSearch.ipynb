{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE534_Project2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urjits25/visual-search/blob/master/CSE534-VisualSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7YLRLTBwYmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5bcdc2f-348d-43c6-f73e-5d2fd7f1068e"
      },
      "source": [
        "# Importing required libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy import stats\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc4TAeq6nb7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SET THE INPUT ARGUMENTS HERE\n",
        "\n",
        "# Dataset stored in Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"./gdrive/My Drive/paris/\"\n",
        "img_list = os.listdir(path)\n",
        "\n",
        "query_img = cv2.imread(path+img_list[100],0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRwIGaUq0caj",
        "colab_type": "text"
      },
      "source": [
        "##Pre-processing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD7Mr4nM0jzh",
        "colab_type": "text"
      },
      "source": [
        "### Feature Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUMlwzC50oXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each image in the dataset\n",
        "# Detect Features and Descriptors using ORB\n",
        "# Cluster the descriptors for all the images\n",
        "# Create a visual vocabulary for each image (vector for each cluster associated with the frequency of each cluster)\n",
        "# Compute frequency for each cluster, and compute Histogram\n",
        "start = time.clock()\n",
        "\n",
        "# For every image in the list\n",
        "for image in img_list: \n",
        "    img = cv2.imread(path+image, 0)\n",
        "    \n",
        "    # Initiate ORB detector\n",
        "    orb = cv2.ORB_create()\n",
        "    \n",
        "    # Detect and describe keypoints with ORB\n",
        "    kp = orb.detect(img, None)\n",
        "    kp, des = orb.compute(img, kp)\n",
        "    \n",
        "    # Write all the descriptors to a file (pre-process and save) \n",
        "    # 500 features per image, 32-dimensional vector for each descriptor\n",
        "    if des is not None: \n",
        "        with open('final_des.txt', 'a+') as f1:\n",
        "            for list in des:\n",
        "                for descr in list:\n",
        "                    f1.write(str(descr) + \" \")\n",
        "                f1.write(\"\\n\")\n",
        "        f1.close()\n",
        "timer = time.clock()-start\n",
        "\n",
        "print(\"--- Creating Descriptors File: %.5f s ---\" % timer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8qTZZ5uHGZT",
        "colab_type": "text"
      },
      "source": [
        "### Feature Clustering - MiniBatchKMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fScwPO9Mwjfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating Descriptors' List for Clustering\n",
        "final_des = []\n",
        "\n",
        "with open('final_des.txt', 'r') as f2:\n",
        "    for line in f2: \n",
        "        final_des.append( [int(ele) for ele in line.split() if ele is not \" \"] )\n",
        "f2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z12tVHtBtDE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_des = np.asarray(final_des)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vJXfhFg6eq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stats.describe(final_des)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78KV5k2y8xm",
        "colab_type": "code",
        "outputId": "5e750a8d-8ebb-4ede-e329-6be4553b7d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start=time.clock()\n",
        "\n",
        "# Change number of clusters to sqrt(len(descriptors))\n",
        "# iterations to 10-30\n",
        "\n",
        "clusters = MiniBatchKMeans(n_clusters= len(img_list)//5, \\\n",
        "                           random_state=0, max_iter=5, \\\n",
        "                           batch_size=len(final_des)//10).fit(final_des)\n",
        "timer = time.clock()-start\n",
        "print(\"--- Clustering descriptors: %.5f s ---\" % timer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Clustering descriptors: 2192.48754 s ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDTNKlKR4FWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving created KMeans model for future use\n",
        "with open('MBKmeans_v2.sav', 'wb') as f3:\n",
        "    pickle.dump(clusters, f3)\n",
        "f3.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hhnb3JYJcUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stats.describe(clusters.cluster_centers_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoEoCd1fYbvu",
        "colab_type": "text"
      },
      "source": [
        "### Visual Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsU3KAKJlXa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('MBKmeans.sav', 'rb') as f3:\n",
        "    clusters = pickle.load(f3)\n",
        "f3.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY9fScRSb5jd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_histogram(descriptor_list, cluster_alg):\n",
        "    histogram = np.zeros(len(cluster_alg.cluster_centers_))\n",
        "    cluster_result=cluster_alg.predict(np.asarray([descriptor_list]))\n",
        "    for i in cluster_result:\n",
        "        histogram[i] += 1.0\n",
        "    return histogram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhPbY61TVnh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For every image in the dataset, do a frequency count of the descriptors\n",
        "start = time.clock()\n",
        "vocab = []\n",
        "\n",
        "with open('vocab.txt', 'a+') as f1:\n",
        "    for des in final_des:\n",
        "        hist = build_histogram(des, clusters)\n",
        "        for count in hist:\n",
        "            f1.write(str(count) + \" \")\n",
        "    f1.write(\"\\n\")\n",
        "f1.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4C2KQj58lsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('vocab.txt', 'r') as f2:\n",
        "    for line in f2: \n",
        "        vocab.append( [int(ele) for ele in line.split() if ele is not \" \"] )\n",
        "f2.close()\n",
        "\n",
        "timer = time.clock() - start\n",
        "print(\"--- Computing Visual Vocabulary: %.5f s ---\" % timer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQadRwHlgCJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = np.asarray(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysA5Vgl0ZEYO",
        "colab_type": "text"
      },
      "source": [
        "## Querying the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M77E05N6ATd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(query_img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpEmZCB26Emi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initiate ORB detector\n",
        "orb1 = cv2.ORB_create()\n",
        "\n",
        "# Detect and describe keypoints with ORB\n",
        "kp = orb1.detect(query_img, None)\n",
        "kp, des = orb1.compute(query_img, kp)\n",
        "\n",
        "# Map all descriptors to their centroids\n",
        "if des is not None: \n",
        "    hist1 = build_histogram(des, clusters)\n",
        "    neighbor = NearestNeighbors(n_neighbors=10).fit(vocab)\n",
        "    dist, result = neighbor.kneighbors([hist1])\n",
        "else:\n",
        "    print(\"Corrupted image\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sExRgQgK7eKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index in result.flatten():\n",
        "    plt.imshow(cv2.imread(path+img_list[index], 0))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW3G1UNsLifF",
        "colab_type": "text"
      },
      "source": [
        "## References: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybIJA0lcyzPq",
        "colab_type": "text"
      },
      "source": [
        "* Python3 Documentation: https://docs.python.org/3/\n",
        "* Bag of Words Example: https://medium.freecodecamp.org/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04<br>\n",
        "* Bag of Visual Words In a Nutshell: https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb<br>\n",
        "* Importing Data from Google Drive into Colab: https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92<br>\n",
        "* Reading all files from a directory: https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory<br>\n",
        "* Comparison of Feature Detectors: https://computer-vision-talks.com/2011-07-13-comparison-of-the-opencv-feature-detection-algorithms/ <br>\n",
        "* ORB Feature Detector: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_orb/py_orb.html#orb<br>\n",
        "* Read line-by-line into a list: https://stackoverflow.com/questions/3277503/how-to-read-a-file-line-by-line-into-a-list<br>\n",
        "* MiniBatchKMeans Clustering: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html<br>\n",
        "* Saving trained model: https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/ <br>\n",
        "* KNN for Ranking Image results: https://scikit-learn.org/stable/modules/neighbors.html\n"
      ]
    }
  ]
}